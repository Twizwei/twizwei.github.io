---
---


@STRING{CVPR = {CVPR}}
@STRING{ECCV = {ECCV}}
@STRING{ICCV = {ICCV}}
@STRING{NEURIPS = {NeurIPS}}
@STRING{ARXIV = {arXiv}}
@STRING{ICLR = {ICLR}}

@article{hong2025relicinteractivevideoworld,
  title        = {RELIC: Interactive Video World Model with Long-Horizon Memory},
  author       = {Yicong Hong and Yiqun Mei and Chongjian Ge and Yiran Xu and Yang Zhou and Sai Bi and Yannick Hold-Geoffroy and Mike Roberts and Matthew Fisher and Eli Shechtman and Kalyan Sunkavalli and Feng Liu and Zhengqi Li and Hao Tan},
  journal    = {Technical Report},
  year         = {2025},
  html         = {https://relic-worldmodel.github.io/},
  pdf          = {https://arxiv.org/abs/2512.04040},
  img          = {assets/img/publications/relic_teaser.mp4},
  tldr         = {RELIC is an interactive video world model that can generate 20s videos in 16FPS with long-horizon memory.},
}

@article{liao2025pad3r,
        author    = {Liao, Ting-Hsuan and Liu, Haowen and Xu, Yiran and Ge, Songwei and Yang, Gengshan and Huang, Jia-Bin},
        title     = {PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos},
        journal   = {SIGGRAPH ASIA},
        year      = {2025},
        html      = {https://pad3r.github.io/},
        pdf       = {https://arxiv.org/abs/2509.25183},
        img       = {assets/img/publications/demo_pad3r.mp4},
        tldr      = {PAD3R reconstructs a dynamic 3D object from a single casual video.},
      } 

@inproceedings{xu2024videogigagan,
  title={VideoGigaGAN: Towards Detail-rich Video Super-Resolution},
  author={Xu, Yiran and Park, Taesung and Zhang, Richard and Zhou, Yang and Shechtman, Eli and Liu, Feng and Huang, Jia-Bin and Liu, Difan},
  booktitle=CVPR,
  year={2025},
  html = {https://videogigagan.github.io/},
  pdf = {https://arxiv.org/abs/2404.12388},
  img = {assets/img/publications/videogigagan_teaser.mp4},
  supp={https://videogigagan.github.io/assets/supp.pdf},
  tldr={VideoGigaGAN is a Video Super-Resolution (VSR) method capable of upscaling videos by up to 8x with fine-grained details.},
  media={https://www.theverge.com/2024/4/24/24138979/adobe-videogigagan-ai-video-upscaling-project-blurry-hd},
}


@inproceedings{lee2025cropper,
  title={Cropper: Vision-Language Model for Image Cropping through In-Context Learning},
  author={Lee*, Seung Hyun and Jiang*, Jijun and Xu*, Yiran and Li*, Zhuofang and Ke, Junjie and Li, Yinxiao and He, Junfeng and Hickson, Steven and Datsenko, Katie and Kim, Sangpil and Yang, Ming-Hsuan and Essa, Irfan and Yang, Feng},
  booktitle=CVPR,
  year={2025},
  html = {},
  pdf = {https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Cropper_Vision-Language_Model_for_Image_Cropping_through_In-Context_Learning_CVPR_2025_paper.pdf},
  img = {assets/img/publications/cropper.jpg},
  tldr={Cropper leverages VLMs for image cropping through in-context learning.},
}

@inproceedings{xie2024flashsplat,
  title={Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats},
  author={Xie, Mingyang and Cai, Haoming and Shah, Sachin and Xu, Yiran and Feng, Brandon Y and Huang, Jia-Bin and Metzler, Christopher A},
  booktitle=ECCV,
  year={2024},
  html = {https://flash-splat.github.io/},
  pdf = {https://neural-fields-beyond-cams.github.io/accepted_papers/6.pdf},
  img = {assets/img/publications/flashsplat.jpg},
  tldr={Flash-Splat is a 3D reflection removal method that leverages flash cues and Gaussian splats for robust performance.},
}

@inproceedings{xu2024innout,
  title={In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing},
  author={Xu, Yiran and Shu, Zhixin and Smith, Cameron and Oh, Seoung Wug and Huang, Jia-Bin},
  booktitle=CVPR,
  year={2024},
  html={https://in-n-out-3d.github.io/},
  pdf={http://arxiv.org/abs/2302.04871},
  img={assets/img/publications/in-n-out_teaser.mp4},
  supp={https://in-n-out-3d.github.io/static/images/supp.pdf},
  tldr={In-N-Out is a 3D GAN inversion method that decomposes the input into two components, enabling effective inversion for Out-of-Distribution (OOD) data.},
}

@inproceedings{qiao2023dynamic,
  title={Dynamic Mesh-aware Radiance Fields},
  author={Qiao, Yi-Ling and Gao, Alexander and Xu, Yiran and Feng, Yue and Huang, Jia-Bin and Lin, Ming},
  booktitle=ICCV,
  year={2023},
  html={https://mesh-aware-rf.github.io/},
  pdf={https://arxiv.org/abs/2309.04581},
  img={assets/img/publications/dmrf.gif},
  tldr={DMRF is a hybrid 3D representation that facilitates physical simulations based on NeRFs},
}

@article{liao2023text,
  title={Text-driven Visual Synthesis with Latent Diffusion Prior},
  author={Liao, Ting-Hsuan and Ge, Songwei and Xu, Yiran and Lee, Yao-Chih and AlBahar, Badour and Huang, Jia-Bin},
  journal=ARXIV,
  year={2023},
  html={https://latent-diffusion-prior.github.io/},
  pdf={https://arxiv.org/abs/2302.08510},
  img={assets/img/publications/latent-diffusion-prior.gif},
  tldr={We leverage latent diffusion priors for multiple text-driven visual synthesis tasks.},
}

@inproceedings{xu2022temporally,
  title={Temporally Consistent Semantic Video Editing},
  author={Xu, Yiran and AlBahar, Badour and Huang, Jia-Bin},
  booktitle=ECCV,
  year={2022},
  html={https://video-edit-gan.github.io/},
  pdf={https://arxiv.org/abs/2206.10590},
  img={assets/img/publications/videogan_teaser.gif},
  tldr={We propose a flow-based video editing method that enables semantic manipulation with temporal consistency.},
}

@inproceedings{xu2020explainable,
  title={Explainable Object-Induced Action Decision for Autonomous Vehicles},
  author={Xu, Yiran and Yang, Xiaoyin and Gong, Lihang and Lin, Hsuan-Chu and Wu, Tz-Ying and Li, Yunsheng and Vasconcelos, Nuno},
  booktitle=CVPR,
  year={2020},
  html={https://twizwei.github.io/bddoia_project/},
  pdf={https://arxiv.org/abs/2003.09405},
  img={assets/img/publications/xoia.png},
  tldr={We propose an explainable decision-making framework for autonomous vehicles.},
}
