
    <!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <script src="https://kit.fontawesome.com/d0f99e6e70.js" crossorigin="anonymous"></script>
  <title>Yiran Xu</title>
  <link rel="icon" type="image/x-icon" href="assets/favicon.ico">
</head>

<body>
    <!-- Navigation bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="https://twizwei.github.io/"><i class="fas fa-home"></i> Yiran Xu</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="#bio">Bio</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#publications">Publications</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#service">Service</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#misc">Miscellaneous</a>
            </li>
          </ul>
        </div>
      </nav>
      
    <div class="container" style="margin-top: 80px;">
        <div class="row">
            <div class="col-md-1"></div>
            <div class="col-md-12">
                <div class="row" style="margin-top: 3em;">
                    <div class="col-sm-12" style="margin-bottom: 1em;">
                    <h3 class="display-4" style="text-align: center;"><span style="font-weight: bold;">Yiran</span> Xu</h3>
                    </div>
                    <br>
                    <div class="col-md-8 text-justify" style="" id="bio">
                        
                <p>
                    I'm a research scientist at <a href="https://research.adobe.com/">Adobe Research</a>. 
                    I received my Ph.D. from the <a href="https://umd.edu/">University of Maryland, College Park</a>, 
                    where I was advised by Prof. <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>.
                </p>
                <p>
                    My research interests are in computer vision and deep learning, focusing on generative models and their applications, especially in <b>videos</b>.
                </p>
                <p>
                    I've interned at <sup><img src="./assets/img/gdm_logos.svg" width="20"/></sup><a href="https://deepmind.google/">Google DeepMind</a>,
                    <sup><img src="./assets/img/adobe_logo.png" width="15"/></sup><a href="https://research.adobe.com/">Adobe Research</a> 
                    and <sup><img src="./assets/img/snapchat-logo.svg" width="20"/></sup><a href="https://research.snap.com/">Snap Research</a>, 
                    where I had the pleasure of collaborating with many<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapsecollaborator" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -4px;">talented researchers.</button>
                    <div class="collapse" id="collapsecollaborator">
                        <div class="card card-body">
                                <p>
                                    <sup><img src="./assets/img/gdm_logos.svg" width="35"/></sup> Google DeepMind (2024): 
                                    <a href="https://sites.google.com/view/feng-yang">Feng Yang</a>,
                                    <a href="https://research.google/people/yinxiaoli/?&type=google">Yinxiao Li</a>,
                                    <a href="https://scholar.google.com/citations?user=fKBmhcUAAAAJ&hl=en">Luciano Sbaiz</a>,
                                    <a href="https://scholar.google.com/citations?user=y0sdeykAAAAJ&hl=en">Junjie Ke</a>,
                                    <a href="https://scholar.google.com/citations?user=j99nvycAAAAJ&hl=en">Miaosen Wang</a>,
                                    <a href="https://scholar.google.com/citations?user=72jdrSUAAAAJ&hl=en">Hang Qi</a>,
                                    <a href="https://sites.google.com/view/hanzhang">Han Zhang</a>,
                                    <a href="https://jlezama.github.io/">Jose Lezama</a>,
                                    <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
                                    <a href="https://www.irfanessa.gatech.edu/">Irfan Essa</a>,
                                    <a href="https://scholar.google.com/citations?user=CSHNLDcAAAAJ&hl=en">Jesse Berent</a>.
                                </p>
                                <p>
                                    <sup><img src="./assets/img/adobe_logo.png" width="35"/></sup> Adobe Research (2023): 
                                    <a href="https://difanliu.github.io/">Difan Liu</a>,
                                    <a href="https://taesung.me/">Taesung Park</a>,
                                    <a href="https://richzhang.github.io/">Richard Zhang</a>,
                                    <a href="https://yzhou359.github.io/">Yang Zhou</a>,
                                    <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                                    <a href="https://pages.cs.wisc.edu/~fliu/">Feng Liu</a>.
                                </p>
                                <p>
                                    <sup><img src="./assets/img/adobe_logo.png" width="35"/></sup> Adobe Research (2022): 
                                    <a href="https://sites.google.com/view/seoungwugoh/">Seoung Wug Oh</a>,
                                    <a href="https://zhixinshu.github.io/">Zhixin Shu</a>,
                                    <a href="https://research.adobe.com/person/cameron-smith/">Cameron Smith</a>.
                                </p>
                                <p>
                                    <sup><img src="./assets/img/snapchat-logo.svg" width="35"/></sup> Snap Research (2021): 
                                    <a href="https://alanspike.github.io/">Jian Ren</a>,
                                    <a href="https://zeng.science/">Zeng Huang</a>,
                                    <a href="https://mlchai.com/">Menglei Chai</a>,
                                    <a href="https://kyleolsz.github.io/">Kyle Olszewski</a>,
                                    <a href="http://hsinyinglee.com/">Hsin-Ying Lee</a>,
                                    <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>.
                                </p>
                        </div>
                    </div>
                </p>
                <p>
                    <a href="./assets/other/bio.txt" target="_blank" style="margin-right: 5px"><i class="fa-solid fa-graduation-cap"></i> Bio</a>
                    <a href="./assets/pdf/Yiran_CV.pdf" target="_blank" style="margin-right: 5px"><i class="fa fa-address-card fa-lg"></i> CV</a>
                    <a href="mailto:yiranx@umd.edu" style="margin-right: 5px"><i class="far fa-envelope-open fa-lg"></i> Mail</a>
                    <a href="https://x.com/Xu_Arthas" target="_blank" style="margin-right: 5px"><i class="fa-brands fa-x-twitter"></i>(Twitter)</a>
                    <a href="https://scholar.google.com/citations?user=lyDM3ugAAAAJ&hl=en" target="_blank" style="margin-right: 5px"><i class="fa-solid fa-book"></i> Scholar</a>
                    <a href="http://github.com/twizwei" target="_blank" style="margin-right: 5px"><i class="fab fa-github fa-lg"></i> Github</a>
                    <a href="https://www.linkedin.com/in/yiran-xu-55683816b/" target="_blank" style="margin-right: 5px"><i class="fab fa-linkedin fa-lg"></i> LinkedIn</a>
                </p>
    
                    </div>
                    <div class="col-md-4" style="">
                        <img src="assets/img/profile.JPG" class="img-thumbnail" width="280px" alt="Profile picture">
                    </div>
                </div>
                <div class="row" style="margin-top: 1em;" id="publications">
                    <div class="col-sm-13" style="">
                        <h4>Publications</h4>
                        <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><video autoplay loop muted playsinline class="img-fluid img-thumbnail" alt="Project video" style="width: 100%; height: auto;"><source src="assets/img/publications/relic_teaser.mp4" type="video/mp4"></video></div><div class="col-sm-8"><a href="https://relic-worldmodel.github.io/" target="_blank">RELIC: Interactive Video World Model with Long-Horizon Memory</a> <br>Yicong Hong, Yiqun Mei, Chongjian Ge, <span style="font-weight: bold";>Yiran Xu</span>, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, Kalyan Sunkavalli, Feng Liu, Zhengqi Li, Hao Tan <br><span style="font-style: italic;">Technical Report</span>, 2025 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">RELIC is an interactive video world model that can generate 20s videos in 16FPS with long-horizon memory.</span> <br><a href="https://relic-worldmodel.github.io/" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2512.04040" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapsehong2025relicinteractivevideoworld" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapsehong2025relicinteractivevideoworld"><div class="card card-body"><pre><code>@article{hong2025relicinteractivevideoworld, 
	author = {Yicong Hong and Yiqun Mei and Chongjian Ge and Yiran Xu and Yang Zhou and Sai Bi and Yannick Hold-Geoffroy and Mike Roberts and Matthew Fisher and Eli Shechtman and Kalyan Sunkavalli and Feng Liu and Zhengqi Li and Hao Tan}, 
	title = {RELIC: Interactive Video World Model with Long-Horizon Memory}, 
	journal = {Technical Report}, 
	year = {2025}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><video autoplay loop muted playsinline class="img-fluid img-thumbnail" alt="Project video" style="width: 100%; height: auto;"><source src="assets/img/publications/demo_pad3r.mp4" type="video/mp4"></video></div><div class="col-sm-8"><a href="https://pad3r.github.io/" target="_blank">PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos</a> <br>Ting-Hsuan Liao, Haowen Liu, <span style="font-weight: bold";>Yiran Xu</span>, Songwei Ge, Gengshan Yang, Jia-Bin Huang <br><span style="font-style: italic;">SIGGRAPH ASIA</span>, 2025 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">PAD3R reconstructs a dynamic 3D object from a single casual video.</span> <br><a href="https://pad3r.github.io/" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2509.25183" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapseliao2025pad3r" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapseliao2025pad3r"><div class="card card-body"><pre><code>@article{liao2025pad3r, 
	author = {Ting-Hsuan Liao and Haowen Liu and Yiran Xu and Songwei Ge and Gengshan Yang and Jia-Bin Huang}, 
	title = {PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos}, 
	journal = {SIGGRAPH ASIA}, 
	year = {2025}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><video autoplay loop muted playsinline class="img-fluid img-thumbnail" alt="Project video" style="width: 100%; height: auto;"><source src="assets/img/publications/videogigagan_teaser.mp4" type="video/mp4"></video></div><div class="col-sm-8"><a href="https://videogigagan.github.io/" target="_blank">VideoGigaGAN: Towards Detail-rich Video Super-Resolution</a> <br><span style="font-weight: bold";>Yiran Xu</span>, Taesung Park, Richard Zhang, Yang Zhou, Eli Shechtman, Feng Liu, Jia-Bin Huang, Difan Liu <br><span style="font-style: italic;">CVPR</span>, 2025 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">VideoGigaGAN is a Video Super-Resolution (VSR) method capable of upscaling videos by up to 8x with fine-grained details.</span> <br><a href="https://videogigagan.github.io/" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2404.12388" target="_blank">Paper</a> / <a href="https://videogigagan.github.io/assets/supp.pdf" target="_blank">Supplemental</a> / <a href="https://www.theverge.com/2024/4/24/24138979/adobe-videogigagan-ai-video-upscaling-project-blurry-hd" target="_blank">Media Coverage</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapsexu2024videogigagan" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapsexu2024videogigagan"><div class="card card-body"><pre><code>@inproceedings{xu2024videogigagan, 
	author = {Yiran Xu and Taesung Park and Richard Zhang and Yang Zhou and Eli Shechtman and Feng Liu and Jia-Bin Huang and Difan Liu}, 
	title = {VideoGigaGAN: Towards Detail-rich Video Super-Resolution}, 
	booktitle = {CVPR}, 
	year = {2025}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><img src="assets/img/publications/cropper.jpg" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-8"><a href="" target="_blank">Cropper: Vision-Language Model for Image Cropping through In-Context Learning</a> <br>Seung Hyun Lee*, Jijun Jiang*, <span style="font-weight: bold";>Yiran Xu*</span>, Zhuofang Li*, Junjie Ke, Yinxiao Li, Junfeng He, Steven Hickson, Katie Datsenko, Sangpil Kim, Ming-Hsuan Yang, Irfan Essa, Feng Yang <br><span style="font-style: italic;">*: equal contribution</span> <br><span style="font-style: italic;">CVPR</span>, 2025 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">Cropper leverages VLMs for image cropping through in-context learning.</span> <br><a href="" target="_blank">Project Page</a> / <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Cropper_Vision-Language_Model_for_Image_Cropping_through_In-Context_Learning_CVPR_2025_paper.pdf" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapselee2025cropper" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapselee2025cropper"><div class="card card-body"><pre><code>@inproceedings{lee2025cropper, 
	author = {Seung Hyun Lee* and Jijun Jiang* and Yiran Xu* and Zhuofang Li* and Junjie Ke and Yinxiao Li and Junfeng He and Steven Hickson and Katie Datsenko and Sangpil Kim and Ming-Hsuan Yang and Irfan Essa and Feng Yang}, 
	title = {Cropper: Vision-Language Model for Image Cropping through In-Context Learning}, 
	booktitle = {CVPR}, 
	year = {2025}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><img src="assets/img/publications/flashsplat.jpg" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-8"><a href="https://flash-splat.github.io/" target="_blank">Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats</a> <br>Mingyang Xie, Haoming Cai, Sachin Shah, <span style="font-weight: bold";>Yiran Xu</span>, Brandon Y Feng, Jia-Bin Huang, Christopher A Metzler <br><span style="font-style: italic;">ECCV</span>, 2024 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">Flash-Splat is a 3D reflection removal method that leverages flash cues and Gaussian splats for robust performance.</span> <br><a href="https://flash-splat.github.io/" target="_blank">Project Page</a> / <a href="https://neural-fields-beyond-cams.github.io/accepted_papers/6.pdf" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapsexie2024flashsplat" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapsexie2024flashsplat"><div class="card card-body"><pre><code>@inproceedings{xie2024flashsplat, 
	author = {Mingyang Xie and Haoming Cai and Sachin Shah and Yiran Xu and Brandon Y Feng and Jia-Bin Huang and Christopher A Metzler}, 
	title = {Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats}, 
	booktitle = {ECCV}, 
	year = {2024}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><video autoplay loop muted playsinline class="img-fluid img-thumbnail" alt="Project video" style="width: 100%; height: auto;"><source src="assets/img/publications/in-n-out_teaser.mp4" type="video/mp4"></video></div><div class="col-sm-8"><a href="https://in-n-out-3d.github.io/" target="_blank">In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing</a> <br><span style="font-weight: bold";>Yiran Xu</span>, Zhixin Shu, Cameron Smith, Seoung Wug Oh, Jia-Bin Huang <br><span style="font-style: italic;">CVPR</span>, 2024 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">In-N-Out is a 3D GAN inversion method that decomposes the input into two components, enabling effective inversion for Out-of-Distribution (OOD) data.</span> <br><a href="https://in-n-out-3d.github.io/" target="_blank">Project Page</a> / <a href="http://arxiv.org/abs/2302.04871" target="_blank">Paper</a> / <a href="https://in-n-out-3d.github.io/static/images/supp.pdf" target="_blank">Supplemental</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapsexu2024innout" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapsexu2024innout"><div class="card card-body"><pre><code>@inproceedings{xu2024innout, 
	author = {Yiran Xu and Zhixin Shu and Cameron Smith and Seoung Wug Oh and Jia-Bin Huang}, 
	title = {In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing}, 
	booktitle = {CVPR}, 
	year = {2024}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><img src="assets/img/publications/dmrf.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-8"><a href="https://mesh-aware-rf.github.io/" target="_blank">Dynamic Mesh-aware Radiance Fields</a> <br>Yi-Ling Qiao, Alexander Gao, <span style="font-weight: bold";>Yiran Xu</span>, Yue Feng, Jia-Bin Huang, Ming Lin <br><span style="font-style: italic;">ICCV</span>, 2023 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">DMRF is a hybrid 3D representation that facilitates physical simulations based on NeRFs</span> <br><a href="https://mesh-aware-rf.github.io/" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2309.04581" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapseqiao2023dynamic" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapseqiao2023dynamic"><div class="card card-body"><pre><code>@inproceedings{qiao2023dynamic, 
	author = {Yi-Ling Qiao and Alexander Gao and Yiran Xu and Yue Feng and Jia-Bin Huang and Ming Lin}, 
	title = {Dynamic Mesh-aware Radiance Fields}, 
	booktitle = {ICCV}, 
	year = {2023}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><img src="assets/img/publications/latent-diffusion-prior.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-8"><a href="https://latent-diffusion-prior.github.io/" target="_blank">Text-driven Visual Synthesis with Latent Diffusion Prior</a> <br>Ting-Hsuan Liao, Songwei Ge, <span style="font-weight: bold";>Yiran Xu</span>, Yao-Chih Lee, Badour AlBahar, Jia-Bin Huang <br><span style="font-style: italic;">arXiv</span>, 2023 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">We leverage latent diffusion priors for multiple text-driven visual synthesis tasks.</span> <br><a href="https://latent-diffusion-prior.github.io/" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2302.08510" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapseliao2023text" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapseliao2023text"><div class="card card-body"><pre><code>@article{liao2023text, 
	author = {Ting-Hsuan Liao and Songwei Ge and Yiran Xu and Yao-Chih Lee and Badour AlBahar and Jia-Bin Huang}, 
	title = {Text-driven Visual Synthesis with Latent Diffusion Prior}, 
	journal = {arXiv}, 
	year = {2023}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><img src="assets/img/publications/videogan_teaser.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-8"><a href="https://video-edit-gan.github.io/" target="_blank">Temporally Consistent Semantic Video Editing</a> <br><span style="font-weight: bold";>Yiran Xu</span>, Badour AlBahar, Jia-Bin Huang <br><span style="font-style: italic;">ECCV</span>, 2022 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">We propose a flow-based video editing method that enables semantic manipulation with temporal consistency.</span> <br><a href="https://video-edit-gan.github.io/" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2206.10590" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapsexu2022temporally" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapsexu2022temporally"><div class="card card-body"><pre><code>@inproceedings{xu2022temporally, 
	author = {Yiran Xu and Badour AlBahar and Jia-Bin Huang}, 
	title = {Temporally Consistent Semantic Video Editing}, 
	booktitle = {ECCV}, 
	year = {2022}, 
}</pre></code></div></div> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-4"><img src="assets/img/publications/xoia.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-8"><a href="https://twizwei.github.io/bddoia_project/" target="_blank">Explainable Object-Induced Action Decision for Autonomous Vehicles</a> <br><span style="font-weight: bold";>Yiran Xu</span>, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu, Yunsheng Li, Nuno Vasconcelos <br><span style="font-style: italic;">CVPR</span>, 2020 <br><span style="font-weight: bold;">TL;DR:</span> <span style="font-style: italic;">We propose an explainable decision-making framework for autonomous vehicles.</span> <br><a href="https://twizwei.github.io/bddoia_project/" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2003.09405" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapsexu2020explainable" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="collapsexu2020explainable"><div class="card card-body"><pre><code>@inproceedings{xu2020explainable, 
	author = {Yiran Xu and Xiaoyin Yang and Lihang Gong and Hsuan-Chu Lin and Tz-Ying Wu and Yunsheng Li and Nuno Vasconcelos}, 
	title = {Explainable Object-Induced Action Decision for Autonomous Vehicles}, 
	booktitle = {CVPR}, 
	year = {2020}, 
}</pre></code></div></div> </div> </div> </div>
                    </div>
                </div>
                <div class="row" style="margin-top: 1em;" id="service">
                    <div class="col-sm-12" style="">
                        <h4>Service</h4>
                        Reviewer for CVPR'2022-2024, ICCV'2021-2023, ECCV'2022-2024, NeurIPS'2023, ICLR'2024, ICML'2024, WACV'2023-2024, ACCV'2024.
                    </div>
                </div>
                <div class="row" style="margin-top: 3em;" id="misc">
                    <div class="col-sm-12" style="">
                        <h4>Miscellaneous</h4>
                        <p>
                            My Chinese name is 许亦冉. 
                            I was born in <a href="https://en.wikipedia.org/wiki/Hengyang">Hengyang</a>, China, also known as the “Wild Goose City” (雁城). 
                            According to local tales, geese migrating south never go beyond Hengyang because they find the warmest weather here.
                        </p>
                        <p>
                            I have two cats, <a href="./assets/img/bai.jpeg">Bai (白)</a> and <a href="./assets/img/Michelle.jpg">Chelle (锈)</a>.
                            They are important members who keep me company during my PhD journey.
                        </p>
                        <p>
                            I started working on computer vision because I wanted to build a makeup machine for my wife. Well...maybe not that simple.
                        </p>
                        <p>
                            I'm also a newbie in photography. Feel free to check out some of my <a href="./gallery.html">photos</a>!
                        </p>
                    </div>
                </div>
                
                
            </div>
            <div class="col-md-1"></div>
        </div>
    </div>

    

    <footer class="footer bg-dark text-white">
        <div class="container text-center">
            <div class="row" style="margin-top: 1em; margin-bottom: 1em;">
                
            <div class="col-sm-12" style="">
                <p>
                    The template is from <a href="https://github.com/m-niemeyer/m-niemeyer.github.io" target="_blank">Michael Niemeyer</a>. <br>
                </p>
            </div>
    
            </div>
            <div class="container text-center mt-1">
                <a href='https://clustrmaps.com/site/1bzwq' title='Visit tracker'>
                    <img src='https://clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=yiWnwTDj8JyOXMKUf1le9Gm5fa1aNjrszfMFmu4slDw' alt="Visit tracker"/>
                </a>
            </div>  
        </div>
    </footer>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
      integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
      crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
      integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
      crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
      integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
      crossorigin="anonymous"></script>
</body>

</html>
    